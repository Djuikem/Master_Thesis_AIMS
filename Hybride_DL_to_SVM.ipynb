{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djuikem/Master_Thesis_AIMS/blob/main/Hybride_DL_to_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjICnJjH5rc4",
        "outputId": "052ca1ae-f095-4156-c777-50e783635bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary packages for Colab\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q matplotlib\n",
        "!pip install -q torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "h9cSklM6CPLg",
        "outputId": "30531f71-6814-4f3a-f2f1-d855a82a2b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy==2\n",
            "  Downloading numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m839.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.14.0\n",
            "  Downloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of gensim to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading numpy-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "Successfully installed gensim-4.3.2 numpy-2.0.0 scipy-1.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "d86faf8bc06f48729bb1626647343059"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gensim numpy==2 scipy==1.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "zkJXzYnNCQXp",
        "outputId": "4bb41e79-5c47-43f9-d26f-eb049bbf21df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.12.0\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<1.29.0,>=1.22.4 (from scipy==1.12.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.0\n",
            "    Uninstalling numpy-2.0.0:\n",
            "      Successfully uninstalled numpy-2.0.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.0\n",
            "    Uninstalling scipy-1.14.0:\n",
            "      Successfully uninstalled scipy-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b11db0f3243d469fa38159213efaeb4f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install scipy==1.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDD1hU_96IQK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report,  roc_curve, auc\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from gensim.models import Word2Vec # ⚒️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO3OnHLX6caU"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U5AgyO064PL",
        "outputId": "6b924b62-2e58-4f1b-b7bb-6efc79b1b925"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10608, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Textual_data/Combined Data.csv')\n",
        "df, _ = train_test_split(df, test_size=0.8, random_state=42, stratify=df['status'])\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0lE6hj6iC5"
      },
      "source": [
        "# Preprocessing of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EGHp3uX7elR",
        "outputId": "da758b9a-bbba-4157-f551-c6d944c43497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK resources if needed\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Download the punkt_tab data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Load data\n",
        "df = df.dropna(subset=['statement'])  # Drop rows with missing statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwbPWiSU79gl"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "targets = []\n",
        "\n",
        "\n",
        "for index in df.index:  # Iterate over the actual index values\n",
        "  sentence = df.loc[index, 'statement']\n",
        "\n",
        "  sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "  temp = []\n",
        "  words = word_tokenize(sentence)\n",
        "  if len(words) > 3 and len(words) < 200:\n",
        "    for j in word_tokenize(sentence):\n",
        "        temp.append(j.lower())\n",
        "    sentences.append(temp)\n",
        "    targets.append(df.loc[index, 'status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKhDdsuA8ZKE"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences = sentences, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRuRlA1-8g_p"
      },
      "outputs": [],
      "source": [
        "# Function to convert sentences to vectors\n",
        "def sentence_to_vector(sentence):\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    # Return a NumPy array, or an empty array if no words are found\n",
        "    return np.array(vectors) if vectors else np.empty((0, model.vector_size))\n",
        "\n",
        "# Find the maximum length of any sentence\n",
        "max_length = max(len(sentence_to_vector(sentence)) for sentence in sentences)\n",
        "\n",
        "# Transform your text data into vectors\n",
        "X = [sentence_to_vector(text) for text in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOpfHEyY8rB4"
      },
      "outputs": [],
      "source": [
        "# Padding or truncating sequences to ensure uniform length\n",
        "X_padded = []\n",
        "for vec in X:\n",
        "    if vec.size == 0:  # Check if the vector is empty\n",
        "        padded_vec = np.zeros((max_length, 100))  # Create a zero-filled array\n",
        "    else:\n",
        "        padded_vec = np.pad(vec, ((0, max_length - vec.shape[0]), (0, 0)), 'constant') if vec.shape[0] < max_length else vec[:max_length]\n",
        "    X_padded.append(padded_vec)\n",
        "X_padded = np.array(X_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y34cuTbD87Bc"
      },
      "outputs": [],
      "source": [
        "targets = pd.Series(targets)\n",
        "\n",
        "# Personnal Mapping\n",
        "perso_mapping = {'Normal': 0, 'Bipolar': 1, 'Depression':2, 'Anxiety':3, 'Stress':4, 'Suicidal':5, 'Personality disorder':6}\n",
        "y = targets.map(perso_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpfug-kw6WFy"
      },
      "source": [
        "# Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrrSgh8_9TQq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Step 1: Split into Train (70%) and Temp (30%)\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_idx, temp_idx in sss1.split(X_padded, y):\n",
        "    X_train, X_temp = X_padded[train_idx], X_padded[temp_idx]\n",
        "    y_train, y_temp = y[train_idx], y[temp_idx]\n",
        "\n",
        "# Convert y_temp to a NumPy array to allow positional indexing\n",
        "y_temp_np = y_temp.to_numpy()\n",
        "\n",
        "# Step 2: Split Temp into Validation (10%) and Test (20%)\n",
        "# That means: 1/3 of 30% goes to val = 10%, and 2/3 to test = 20%\n",
        "# Use the NumPy array y_temp_np for splitting\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=2/3, random_state=42)\n",
        "for test_idx, val_idx in sss2.split(X_temp, y_temp_np): # Use y_temp_np here\n",
        "    X_test, X_val = X_temp[test_idx], X_temp[val_idx]\n",
        "    # Index the NumPy array y_temp_np directly\n",
        "    y_test, y_val = y_temp_np[test_idx], y_temp_np[val_idx] # Use y_temp_np here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKfjH_PI9loq"
      },
      "outputs": [],
      "source": [
        "if isinstance(y_train, pd.Series):\n",
        "    y_train = y_train.to_numpy().reshape(-1, 1)\n",
        "else:\n",
        "    y_train = y_train.reshape(-1, 1) # Reshape if already a numpy array\n",
        "\n",
        "y_test = y_test.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEw-i1e89xLU"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int32')\n",
        "y_test = y_test.astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dc0yVrI95dg"
      },
      "outputs": [],
      "source": [
        "X_train = tf.convert_to_tensor(np.array(X_train), dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "\n",
        "X_test = tf.convert_to_tensor(np.array(X_test), dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhUgsUtMJaL3"
      },
      "outputs": [],
      "source": [
        "X_train = torch.from_numpy(np.array(X_train)).float()\n",
        "# Convert TensorFlow tensor back to NumPy array before creating PyTorch tensor\n",
        "y_train = torch.tensor(y_train.numpy(), dtype=torch.long) # Use torch.long for classification targets\n",
        "\n",
        "X_test = torch.from_numpy(np.array(X_test)).float()\n",
        "# Convert TensorFlow tensor back to NumPy array before creating PyTorch tensor\n",
        "y_test = torch.tensor(y_test.numpy(), dtype=torch.long) # Use torch.long for classification targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hM5Vl5m6A63"
      },
      "source": [
        "# Models Training and Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaDP6nzfyP9F"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAazvbyjlQq8",
        "outputId": "71f6e219-ab51-43ee-90dd-1dafa4446823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: torch.Size([5819, 199, 100])\n",
            "Epoch 1, Loss: 231.8526\n",
            "Epoch 2, Loss: 196.7538\n",
            "Epoch 3, Loss: 182.6827\n",
            "Epoch 4, Loss: 174.7077\n",
            "Epoch 5, Loss: 163.7318\n",
            "Epoch 6, Loss: 154.0191\n",
            "Epoch 7, Loss: 145.7646\n",
            "Epoch 8, Loss: 139.6035\n",
            "Epoch 9, Loss: 132.0665\n",
            "Epoch 10, Loss: 126.6004\n",
            "Epoch 11, Loss: 119.6767\n",
            "Epoch 12, Loss: 114.1834\n",
            "Epoch 13, Loss: 106.6189\n",
            "Epoch 14, Loss: 100.4692\n",
            "Epoch 15, Loss: 95.4932\n",
            "Epoch 16, Loss: 89.3482\n",
            "Epoch 17, Loss: 80.5004\n",
            "Epoch 18, Loss: 75.6929\n",
            "Epoch 19, Loss: 73.8552\n",
            "Epoch 20, Loss: 65.5860\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Vérifie que X_train est bien en (B, S, F)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "\n",
        "# Si X_train est en (B, F), on le transforme en (B, 1, F)\n",
        "if X_train.dim() == 2:\n",
        "    X_train = X_train.unsqueeze(1)  # Ajoute une dimension de séquence\n",
        "\n",
        "# ------------------ Classes ------------------\n",
        "\n",
        "class LSTMFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 2,\n",
        "                 bidirectional: bool = True, dropout: float = 0.2, use_pooling: bool = False):\n",
        "        super().__init__()\n",
        "        self.use_pooling = use_pooling\n",
        "        self.bidirectional = bidirectional\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        self.norm = nn.LayerNorm(self.output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.dim() == 3, f\"Expected input of shape (B, S, F), got {x.shape}\"\n",
        "        output, (h_n, _) = self.lstm(x)\n",
        "        if self.bidirectional:\n",
        "            features = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
        "        else:\n",
        "            features = h_n[-1]\n",
        "        return self.norm(features)\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_classes: int,\n",
        "                 num_layers: int = 2, bidirectional: bool = True, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = LSTMFeatureExtractor(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.classifier = nn.Linear(self.feature_extractor.output_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# ------------------ Entraînement ------------------\n",
        "\n",
        "model = LSTMClassifier(input_size=100, hidden_size=64, num_classes=7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train.float(), y_train.long())\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        batch_y = batch_y.squeeze()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgTDNJAVu-CR"
      },
      "outputs": [],
      "source": [
        "feature_model_lstm = model.feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2jOqQQRylWG"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNh9UbJ5R6uy",
        "outputId": "a57d4436-7c83-48cb-eea8-f1ac49d0c83f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 228.5250\n",
            "Epoch 2, Loss: 177.7923\n",
            "Epoch 3, Loss: 162.3538\n",
            "Epoch 4, Loss: 152.0652\n",
            "Epoch 5, Loss: 143.5209\n",
            "Epoch 6, Loss: 136.5296\n",
            "Epoch 7, Loss: 129.0651\n",
            "Epoch 8, Loss: 124.0574\n",
            "Epoch 9, Loss: 116.1854\n",
            "Epoch 10, Loss: 110.2839\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved GRU-based feature extractor with optional bidirectionality, pooling, dropout, and normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_size: int,\n",
        "                 num_layers: int = 2,\n",
        "                 bidirectional: bool = True,\n",
        "                 dropout: float = 0.2,\n",
        "                 use_pooling: bool = False):\n",
        "        super().__init__()\n",
        "        self.use_pooling = use_pooling\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        self.output_size = hidden_size * (2 if bidirectional else 1)\n",
        "        self.norm = nn.LayerNorm(self.output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.dim() == 3, f\"Expected input of shape (B, S, F), got {x.shape}\"\n",
        "\n",
        "        output, h_n = self.gru(x)\n",
        "\n",
        "        if self.use_pooling:\n",
        "            # Moyenne temporelle (pooling sur toute la séquence)\n",
        "            features = output.mean(dim=1)\n",
        "        else:\n",
        "            # Use the last hidden state(s) from the last layer\n",
        "            if self.bidirectional:\n",
        "                # Concatenate forward and backward hidden states from the last layer\n",
        "                features = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
        "            else:\n",
        "                # Use the last hidden state of the last layer for unidirectional\n",
        "                features = h_n[-1, :, :]\n",
        "\n",
        "        return self.norm(features)\n",
        "\n",
        "\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, num_classes: int,\n",
        "                 num_layers: int = 2, bidirectional: bool = True, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = GRUFeatureExtractor(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.classifier = nn.Linear(self.feature_extractor.output_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# Modèle\n",
        "model1 = GRUClassifier(input_size=100, hidden_size=64, num_classes=7)  # Ajuste input_size\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train.float(), y_train.long().squeeze())\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement\n",
        "for epoch in range(10):\n",
        "    model1.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model1(batch_x)\n",
        "        batch_y = batch_y.squeeze()  # just in case\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvkpoMftv4jb"
      },
      "outputs": [],
      "source": [
        "feature_model_GRU = model1.feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTic-1EUypNB"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyUvzptQR-yO",
        "outputId": "d62dc610-a548-49f7-e75d-12064f765be0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 231.9998\n",
            "Epoch 2, Loss: 188.9344\n",
            "Epoch 3, Loss: 176.2791\n",
            "Epoch 4, Loss: 166.5117\n",
            "Epoch 5, Loss: 158.7657\n",
            "Epoch 6, Loss: 151.0386\n",
            "Epoch 7, Loss: 146.1492\n",
            "Epoch 8, Loss: 146.6983\n",
            "Epoch 9, Loss: 141.7597\n",
            "Epoch 10, Loss: 137.2701\n",
            "Epoch 11, Loss: 135.8291\n",
            "Epoch 12, Loss: 129.2512\n",
            "Epoch 13, Loss: 125.2320\n",
            "Epoch 14, Loss: 125.5763\n",
            "Epoch 15, Loss: 121.3606\n",
            "Epoch 16, Loss: 120.4887\n",
            "Epoch 17, Loss: 115.5848\n",
            "Epoch 18, Loss: 115.0113\n",
            "Epoch 19, Loss: 111.2712\n",
            "Epoch 20, Loss: 109.3387\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-np.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, S, D)\n",
        "        return x + self.pe[:, :x.size(1)].to(x.dtype)\n",
        "\n",
        "\n",
        "class TransformerFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based feature extractor with flexible pooling and improved embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 d_model: int = 64,\n",
        "                 nhead: int = 4,\n",
        "                 num_layers: int = 2,\n",
        "                 max_len: int = 500,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 pooling: str = 'mean'  # options: 'mean', 'max', 'cls'\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.pooling = pooling\n",
        "\n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Linear(input_size, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                   nhead=nhead,\n",
        "                                                   dropout=dropout_rate,\n",
        "                                                   batch_first=False)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.output_size = d_model\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.dim() == 3, f\"Expected input of shape (B, S, F), got {x.shape}\"\n",
        "\n",
        "        x = self.embedding(x)           # (B, S, D)\n",
        "        x = self.pos_encoder(x)         # (B, S, D)\n",
        "        x = x.permute(1, 0, 2)          # (S, B, D)\n",
        "        x = self.transformer(x)         # (S, B, D)\n",
        "        x = x.permute(1, 0, 2)          # (B, S, D)\n",
        "\n",
        "        if self.pooling == 'mean':\n",
        "            features = x.mean(dim=1)\n",
        "        elif self.pooling == 'max':\n",
        "            features, _ = x.max(dim=1)\n",
        "        elif self.pooling == 'cls':\n",
        "            features = x[:, 0, :]  # suppose que le token [CLS] est le premier\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown pooling method: {self.pooling}\")\n",
        "\n",
        "        return self.dropout(features)\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_size: int, num_classes: int,\n",
        "                 d_model: int = 64, nhead: int = 4, num_layers: int = 2,\n",
        "                 dropout_rate: float = 0.1, pooling: str = 'mean'):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = TransformerFeatureExtractor(\n",
        "            input_size=input_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_layers,\n",
        "            dropout_rate=dropout_rate,\n",
        "            pooling=pooling\n",
        "        )\n",
        "        self.classifier = nn.Linear(self.feature_extractor.output_size, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        features = self.feature_extractor(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# Création du modèle\n",
        "model2 = TransformerClassifier(input_size=100, num_classes=7)\n",
        "\n",
        "# Fonction de perte et optimiseur\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train.float(), y_train.long())\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Entraînement\n",
        "for epoch in range(20):\n",
        "    model2.train()\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(batch_x)\n",
        "        batch_y = batch_y.squeeze()  # S'assurer que les labels sont 1D\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoFbR30bxeTR"
      },
      "outputs": [],
      "source": [
        "feature_model_Transformer = model2.feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0FkH2Q6PCJ"
      },
      "source": [
        "# Classification with SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6geKxgnEjjF"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "def extract_features(model, X_tensor, batch_size=32):\n",
        "    \"\"\"\n",
        "    Extrait les caractéristiques d'un modèle PyTorch.\n",
        "    Entrée : tenseur de forme (N, S, F)\n",
        "    Sortie : numpy array de caractéristiques extraites\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dataset = DataLoader(X_tensor, batch_size=batch_size)\n",
        "\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataset:\n",
        "            outputs = model(batch)\n",
        "            features.append(outputs)\n",
        "\n",
        "    return torch.cat(features, dim=0).numpy()\n",
        "\n",
        "def train_feature_svm(feature_model, kernel='rbf', C=10, gamma='scale'):\n",
        "    \"\"\"\n",
        "    Entraîne un SVM basé sur les features extraites d'un modèle deep learning.\n",
        "    Affiche un rapport de classification et retourne les métriques principales.\n",
        "    \"\"\"\n",
        "    # Extraire les caractéristiques à partir du modèle\n",
        "    features_train = extract_features(feature_model, X_train)\n",
        "    features_test = extract_features(feature_model, X_test)\n",
        "\n",
        "    #Recherche d'hyperparametres\n",
        "    param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
        "    grid = GridSearchCV(SVC(probability=True), param_grid, cv=3, scoring='f1_weighted')\n",
        "    grid.fit(features_train, y_train)\n",
        "    print(grid.best_params_)\n",
        "    C = grid.best_params_['C']\n",
        "    gamma = grid.best_params_['gamma']\n",
        "\n",
        "    # Entraîner le SVM\n",
        "    svm = SVC(kernel=kernel, C=C, gamma=gamma, probability=True)\n",
        "    svm.fit(features_train, y_train)\n",
        "    y_pred = svm.predict(features_test)\n",
        "\n",
        "    # Affichage du rapport\n",
        "    print(\"\\n✅ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"\\n📊 Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Retour des scores\n",
        "    return {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
        "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
        "        'F1': f1_score(y_test, y_pred, average='weighted')\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB1uNrTzEzTP",
        "outputId": "3b8c794e-bbed-4205-f130-f340aa4892b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       291\n",
            "           1       0.43      0.39      0.41        38\n",
            "           2       0.63      0.63      0.63       221\n",
            "           3       0.46      0.42      0.44        57\n",
            "           4       0.33      0.28      0.30        46\n",
            "           5       0.57      0.59      0.58       163\n",
            "           6       0.50      0.07      0.12        15\n",
            "\n",
            "    accuracy                           0.66       831\n",
            "   macro avg       0.54      0.47      0.48       831\n",
            "weighted avg       0.65      0.66      0.65       831\n",
            "\n",
            "\n",
            "📊 Confusion Matrix:\n",
            " [[263   1   6   6   6   9   0]\n",
            " [  1  15   7  10   3   1   1]\n",
            " [ 14   3 139   3   2  60   0]\n",
            " [  8  10   4  24  11   0   0]\n",
            " [ 11   2  12   7  13   1   0]\n",
            " [ 13   0  52   1   1  96   0]\n",
            " [  2   4   1   1   4   2   1]]\n",
            "LSTM → SVM Results: {'Accuracy': 0.6630565583634176, 'Precision': 0.6521458477651202, 'Recall': 0.6630565583634176, 'F1': 0.6540251440129488}\n"
          ]
        }
      ],
      "source": [
        "# lstm_model = LSTMFeatureExtractor(input_size=100, hidden_size=256)\n",
        "lstm_results = train_feature_svm(feature_model_lstm)\n",
        "print(\"LSTM → SVM Results:\", lstm_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npRE0UFZFAQV",
        "outputId": "5279bb03-59c4-4f2b-b1d0-ff8f740ad60d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88       291\n",
            "           1       0.38      0.34      0.36        38\n",
            "           2       0.71      0.59      0.65       221\n",
            "           3       0.48      0.49      0.49        57\n",
            "           4       0.44      0.41      0.43        46\n",
            "           5       0.58      0.71      0.64       163\n",
            "           6       0.00      0.00      0.00        15\n",
            "\n",
            "    accuracy                           0.69       831\n",
            "   macro avg       0.49      0.49      0.49       831\n",
            "weighted avg       0.67      0.69      0.68       831\n",
            "\n",
            "\n",
            "📊 Confusion Matrix:\n",
            " [[266   1   3   4   5  12   0]\n",
            " [  1  13   7  10   6   1   0]\n",
            " [ 13   1 131   6   2  68   0]\n",
            " [  8  11   1  28   8   1   0]\n",
            " [  8   5   8   6  19   0   0]\n",
            " [ 14   0  33   1   0 115   0]\n",
            " [  3   3   2   3   3   1   0]]\n",
            "GRU → SVM Results: {'Accuracy': 0.6883273164861613, 'Precision': 0.6748969410107953, 'Recall': 0.6883273164861613, 'F1': 0.6785759695992346}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# gru_model = GRUFeatureExtractor(input_size=100, hidden_size=256)\n",
        "gru_results = train_feature_svm(feature_model_GRU)\n",
        "print(\"GRU → SVM Results:\", gru_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWHbjgrqFBR7",
        "outputId": "dbffd16f-eb51-41b7-88bd-646e30cda44d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90       291\n",
            "           1       0.78      0.74      0.76        38\n",
            "           2       0.74      0.66      0.70       221\n",
            "           3       0.75      0.81      0.78        57\n",
            "           4       0.65      0.57      0.60        46\n",
            "           5       0.61      0.72      0.66       163\n",
            "           6       0.56      0.33      0.42        15\n",
            "\n",
            "    accuracy                           0.76       831\n",
            "   macro avg       0.71      0.68      0.69       831\n",
            "weighted avg       0.76      0.76      0.76       831\n",
            "\n",
            "\n",
            "📊 Confusion Matrix:\n",
            " [[265   0   7   2   6  11   0]\n",
            " [  1  28   4   3   2   0   0]\n",
            " [  8   5 145   1   1  60   1]\n",
            " [  2   2   0  46   4   1   2]\n",
            " [  5   1   5   7  26   1   1]\n",
            " [ 13   0  33   0   0 117   0]\n",
            " [  4   0   2   2   1   1   5]]\n",
            "Transformer → SVM Results: {'Accuracy': 0.7605294825511432, 'Precision': 0.7616013582791235, 'Recall': 0.7605294825511432, 'F1': 0.758785548392477}\n"
          ]
        }
      ],
      "source": [
        "# transformer_model = TransformerFeatureExtractor(input_size=100, d_model=256, nhead=8, num_layers=3) # Changed input_size from 20 to 100\n",
        "transformer_results = train_feature_svm(feature_model_Transformer)\n",
        "print(\"Transformer → SVM Results:\", transformer_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KOtWTmhto83"
      },
      "source": [
        "# Hybride-parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpUwenA-1WZX",
        "outputId": "e537d162-4440-4d8f-8274-a2d82fbb9aee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8313, 19900)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# reshape\n",
        "X_padded = X_padded.reshape(X_padded.shape[0], -1)\n",
        "X_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgr0OBt01_l2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Split into Train (70%) and Temp (30%)\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
        "for train_idx, temp_idx in sss1.split(X_padded, y):\n",
        "    X_train2, X_temp = X_padded[train_idx], X_padded[temp_idx]\n",
        "    y_train2, y_temp = y[train_idx], y[temp_idx]\n",
        "\n",
        "# Convert y_temp to a NumPy array to allow positional indexing\n",
        "y_temp_np = y_temp.to_numpy()\n",
        "\n",
        "# Step 2: Split Temp into Validation (10%) and Test (20%)\n",
        "# That means: 1/3 of 30% goes to val = 10%, and 2/3 to test = 20%\n",
        "# Use the NumPy array y_temp_np for splitting\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=2/3, random_state=42)\n",
        "for test_idx, val_idx in sss2.split(X_temp, y_temp_np): # Use y_temp_np here\n",
        "    X_test2, X_val2 = X_temp[test_idx], X_temp[val_idx]\n",
        "    # Index the NumPy array y_temp_np directly\n",
        "    y_test2, y_val2 = y_temp_np[test_idx], y_temp_np[val_idx] # Use y_temp_np here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwWSwqnD2JcN",
        "outputId": "b4918886-9466-45ae-9873-55831bb950a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (5819, 19900)\n",
            "X_test shape: (831, 19900)\n",
            "Max token value: 3.233741\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"X_train shape:\", X_train2.shape)\n",
        "print(\"X_test shape:\", X_test2.shape)\n",
        "print(\"Max token value:\", np.max(X_train2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_y79KKEtu9r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ensemble: Average probabilities from SVM and LSTM classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "'''\n",
        "#Recherche d'hyperparametres\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['rbf', 'linear']}\n",
        "grid = GridSearchCV(SVC(probability=True), param_grid, cv=3, scoring='f1_weighted')\n",
        "grid.fit(X_train2, y_train2)\n",
        "print(grid.best_params_)\n",
        "C = grid.best_params_['C']\n",
        "gamma = grid.best_params_['gamma']\n",
        "'''\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
        "svm.fit(X_train2, y_train2)\n",
        "y_pred = svm.predict(X_test2)\n",
        "# prob_svm = svm.predict_proba(X_test2)[:, 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUIZlBCQQEWY"
      },
      "outputs": [],
      "source": [
        "prob_svm = svm.predict_proba(X_test2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqjl9nUJxR5c",
        "outputId": "07ba0dcf-a2b0-4c24-dff3-5a99acd670d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prob_svm: (831, 7)\n",
            "Shape of prob_transformer: (831, 7)\n",
            "Ensemble accuracy: 0.7388688327316486\n",
            "Precision: 0.7403\n",
            "Recall:    0.7389\n",
            "F1-score:  0.7305\n",
            "\n",
            "📊 Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8770    0.9313    0.9033       291\n",
            "           1     0.5556    0.7895    0.6522        38\n",
            "           2     0.6833    0.6833    0.6833       221\n",
            "           3     0.6216    0.8070    0.7023        57\n",
            "           4     0.6087    0.3043    0.4058        46\n",
            "           5     0.6712    0.6012    0.6343       163\n",
            "           6     1.0000    0.2667    0.4211        15\n",
            "\n",
            "    accuracy                         0.7389       831\n",
            "   macro avg     0.7168    0.6262    0.6289       831\n",
            "weighted avg     0.7403    0.7389    0.7305       831\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 📌 TensorFlow/Keras imports\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Conv1D, Flatten, Dropout, MultiHeadAttention, LayerNormalization, Input, Reshape, Add, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cvxopt  # For optimization in SVM-based models\n",
        "# Import GlobalAveragePooling1D from tensorflow.keras.layers\n",
        "from tensorflow.keras.layers import  GlobalAveragePooling1D, Layer\n",
        "'''\n",
        "class PositionalEncodingLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        pos_encoding = tf.cast(positions[:, tf.newaxis], dtype=tf.float32)\n",
        "        return x + pos_encoding\n",
        "'''\n",
        "\n",
        "def transformer_block(x, num_heads, key_dim, ff_dim=128, dropout_rate=0.1):\n",
        "  attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
        "  attention_output = Dropout(dropout_rate)(attention_output)\n",
        "  attention_output = LayerNormalization(epsilon=1e-6)(attention_output + x)\n",
        "\n",
        "  ff = Dense(ff_dim, activation='relu')(attention_output)\n",
        "  ff = Dropout(dropout_rate)(ff)\n",
        "  ff = Dense(key_dim)(ff)\n",
        "\n",
        "  return LayerNormalization(epsilon=1e-6)(ff + attention_output)\n",
        "\n",
        "def build_transformer(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    key_dim_val = 64  # ↓ Réduction de la dimension d’attention\n",
        "    x = Dense(key_dim_val)(inputs) if input_shape[-1] != key_dim_val else inputs\n",
        "\n",
        "    # x = PositionalEncodingLayer()(x)  # ← Ajout de l'encodage positionnel\n",
        "\n",
        "    x = transformer_block(x, num_heads=2, key_dim=key_dim_val)  # ↓ Moins de têtes\n",
        "    x = transformer_block(x, num_heads=2, key_dim=key_dim_val)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dense(32, activation='relu')(x)  # ↓ Taille de couche intermédiaire\n",
        "    x = Dropout(0.2)(x)\n",
        "    # x = Dense(64, activation='relu')(x)\n",
        "    output = Dense(7, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "transformer_model = build_transformer(X_train.shape[1:])\n",
        "transformer_model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)\n",
        "prob_transformer = transformer_model.predict(X_test, verbose=0)\n",
        "\n",
        "# --- Debugging: Check shapes before combining ---\n",
        "print(\"Shape of prob_svm:\", prob_svm.shape)\n",
        "print(\"Shape of prob_transformer:\", prob_transformer.shape)\n",
        "# --- End Debugging ---\n",
        "\n",
        "# Ensemble\n",
        "# prob_ensemble = (prob_svm[:10] + prob_dl) / 2\n",
        "# pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "# print(\"Ensemble accuracy:\", accuracy_score(y_test, pred_ensemble))\n",
        "\n",
        "if prob_svm.shape == prob_transformer.shape:\n",
        "    prob_ensemble = (prob_svm + prob_transformer) / 2\n",
        "    pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "    print(\"Ensemble accuracy:\", accuracy_score(y_test2, pred_ensemble)) # Use y_test2 for the flattened data split\n",
        "\n",
        "    precision = precision_score(y_test2, pred_ensemble, average='weighted')\n",
        "    recall = recall_score(y_test2, pred_ensemble, average='weighted')\n",
        "    f1 = f1_score(y_test2, pred_ensemble, average='weighted')\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n📊 Ensemble Classification Report:\")\n",
        "    print(classification_report(y_test2, pred_ensemble, digits=4))  # digits=4 for more precision\n",
        "else:\n",
        "    print(\"Error: Probability shapes mismatch for ensemble.\")\n",
        "    print(\"prob_svm shape:\", prob_svm.shape)\n",
        "    print(\"prob_transformer shape:\", prob_transformer.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXXyXBVSe199",
        "outputId": "198b0ca1-bff7-4cd5-c610-ac47ec86ac80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prob_svm: (831, 7)\n",
            "Shape of prob_lstm: (831, 7)\n",
            "Ensemble accuracy: 0.6594464500601684\n",
            "Precision: 0.6348\n",
            "Recall:    0.6594\n",
            "F1-score:  0.6398\n",
            "\n",
            "📊 Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8449    0.9175    0.8797       291\n",
            "           1     0.5000    0.1316    0.2083        38\n",
            "           2     0.5854    0.6516    0.6167       221\n",
            "           3     0.4203    0.5088    0.4603        57\n",
            "           4     0.3462    0.1957    0.2500        46\n",
            "           5     0.5732    0.5767    0.5749       163\n",
            "           6     0.0000    0.0000    0.0000        15\n",
            "\n",
            "    accuracy                         0.6594       831\n",
            "   macro avg     0.4671    0.4260    0.4271       831\n",
            "weighted avg     0.6348    0.6594    0.6398       831\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "def build_lstm(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        LSTM(48, return_sequences=True),\n",
        "        LSTM(32, return_sequences=True),  # 32 neurons and produces an output at each timestep\n",
        "        LSTM(16),  # 16 LSTM units and produces the last output\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(7, activation='softmax')  # Output layer\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "'''\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, LayerNormalization\n",
        "\n",
        "def build_lstm(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))(inputs)\n",
        "    x = Bidirectional(LSTM(48, return_sequences=True, dropout=0.2))(x)\n",
        "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2))(x)\n",
        "    x = Bidirectional(LSTM(16, dropout=0.2))(x)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    outputs = Dense(7, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_model = build_lstm(X_train.shape[1:])\n",
        "lstm_model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
        "prob_lstm = lstm_model.predict(X_test, verbose=0)\n",
        "\n",
        "# --- Debugging: Check shapes before combining ---\n",
        "print(\"Shape of prob_svm:\", prob_svm.shape)\n",
        "print(\"Shape of prob_lstm:\", prob_lstm.shape)\n",
        "# --- End Debugging ---\n",
        "\n",
        "# Ensemble\n",
        "# prob_ensemble = (prob_svm[:10] + prob_dl) / 2\n",
        "# pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "# print(\"Ensemble accuracy:\", accuracy_score(y_test, pred_ensemble))\n",
        "\n",
        "if prob_svm.shape == prob_lstm.shape:\n",
        "    prob_ensemble = (prob_svm + prob_lstm) / 2\n",
        "    pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "    print(\"Ensemble accuracy:\", accuracy_score(y_test2, pred_ensemble)) # Use y_test2 for the flattened data split\n",
        "\n",
        "    precision = precision_score(y_test2, pred_ensemble, average='weighted')\n",
        "    recall = recall_score(y_test2, pred_ensemble, average='weighted')\n",
        "    f1 = f1_score(y_test2, pred_ensemble, average='weighted')\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n📊 Ensemble Classification Report:\")\n",
        "    print(classification_report(y_test2, pred_ensemble, digits=4))  # digits=4 for more precision\n",
        "else:\n",
        "    print(\"Error: Probability shapes mismatch for ensemble.\")\n",
        "    print(\"prob_svm shape:\", prob_svm.shape)\n",
        "    print(\"prob_lstm shape:\", prob_lstm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_svm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPfKtkbIE9On",
        "outputId": "60f1ef1b-fd7d-4be4-a126-e2f475b0aab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03860518, 0.17422501, 0.3045557 , ..., 0.14736951, 0.1182403 ,\n",
              "        0.04671141],\n",
              "       [0.04182473, 0.01988922, 0.58449738, ..., 0.02854623, 0.27169553,\n",
              "        0.01674202],\n",
              "       [0.00442356, 0.00478961, 0.60052767, ..., 0.00879629, 0.35530361,\n",
              "        0.00943918],\n",
              "       ...,\n",
              "       [0.14415066, 0.35562892, 0.13586686, ..., 0.18272559, 0.0539766 ,\n",
              "        0.02449984],\n",
              "       [0.01306834, 0.00629829, 0.44180502, ..., 0.01037911, 0.50788201,\n",
              "        0.01182551],\n",
              "       [0.13167475, 0.13149756, 0.38335882, ..., 0.09501435, 0.15047808,\n",
              "        0.0156604 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wuP1Cy9xoom",
        "outputId": "9784706b-1403-40be-b264-de171fe06f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prob_svm: (831, 7)\n",
            "Shape of prob_gru: (831, 7)\n",
            "Ensemble accuracy: 0.6859205776173285\n",
            "Precision: 0.6704\n",
            "Recall:    0.6859\n",
            "F1-score:  0.6765\n",
            "\n",
            "📊 Ensemble Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8617    0.9210    0.8904       291\n",
            "           1     0.3542    0.4474    0.3953        38\n",
            "           2     0.6371    0.6833    0.6594       221\n",
            "           3     0.6222    0.4912    0.5490        57\n",
            "           4     0.4286    0.3913    0.4091        46\n",
            "           5     0.5946    0.5399    0.5659       163\n",
            "           6     0.0000    0.0000    0.0000        15\n",
            "\n",
            "    accuracy                         0.6859       831\n",
            "   macro avg     0.4998    0.4963    0.4956       831\n",
            "weighted avg     0.6704    0.6859    0.6765       831\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Function to build the GRU model\n",
        "'''\n",
        "def build_gru(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        GRU(64, return_sequences=True),\n",
        "        GRU(48, return_sequences=True),\n",
        "        GRU(32, return_sequences=True),  # 32 neurons and produces an output at each timestep\n",
        "        GRU(16),  # 16 GRU units and produces the last output\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(7, activation='softmax')  # Output layer\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "'''\n",
        "def build_gru(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Bidirectional(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.1))(inputs)\n",
        "    x = Bidirectional(GRU(48, return_sequences=True, dropout=0.2))(x)\n",
        "    x = Bidirectional(GRU(32, return_sequences=True, dropout=0.2))(x)\n",
        "    x = Bidirectional(GRU(16, dropout=0.2))(x)\n",
        "\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)  # couche ajoutée\n",
        "    outputs = Dense(7, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "gru_model = build_gru(X_train.shape[1:])\n",
        "gru_model.fit(X_train, y_train, epochs=50, batch_size=64, verbose=0)\n",
        "prob_gru = gru_model.predict(X_test, verbose=0)\n",
        "\n",
        "# --- Debugging: Check shapes before combining ---\n",
        "print(\"Shape of prob_svm:\", prob_svm.shape)\n",
        "print(\"Shape of prob_gru:\", prob_gru.shape)\n",
        "# --- End Debugging ---\n",
        "\n",
        "# Ensemble\n",
        "# prob_ensemble = (prob_svm[:10] + prob_dl) / 2\n",
        "# pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "# print(\"Ensemble accuracy:\", accuracy_score(y_test, pred_ensemble))\n",
        "\n",
        "if prob_svm.shape == prob_gru.shape:\n",
        "    prob_ensemble = (prob_svm + prob_gru) / 2\n",
        "    pred_ensemble = np.argmax(prob_ensemble, axis=1)\n",
        "    print(\"Ensemble accuracy:\", accuracy_score(y_test2, pred_ensemble)) # Use y_test2 for the flattened data split\n",
        "\n",
        "    precision = precision_score(y_test2, pred_ensemble, average='weighted')\n",
        "    recall = recall_score(y_test2, pred_ensemble, average='weighted')\n",
        "    f1 = f1_score(y_test2, pred_ensemble, average='weighted')\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\n📊 Ensemble Classification Report:\")\n",
        "    print(classification_report(y_test2, pred_ensemble, digits=4))  # digits=4 for more precision\n",
        "else:\n",
        "    print(\"Error: Probability shapes mismatch for ensemble.\")\n",
        "    print(\"prob_svm shape:\", prob_svm.shape)\n",
        "    print(\"prob_gru shape:\", prob_gru.shape)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1F7-EX_By4CseqJq8HlSqYQUH00gwjbjK",
      "authorship_tag": "ABX9TyPDwP1mmR4Fi4Xp5prHPZ9f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}